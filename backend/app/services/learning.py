import os
import json
import yfinance as yf
from datetime import datetime, timedelta
import pandas as pd
from . import googleapi, disclosure, lstm, finance, summarize
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import numpy as np
import re

import warnings # ã„ã£ãŸã‚“
# ç‰¹å®šã®è­¦å‘Šã‚’ç„¡è¦–ã™ã‚‹
warnings.filterwarnings('ignore', category=pd.errors.SettingWithCopyWarning)

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ä¼šç¤¾ãƒªã‚¹ãƒˆã®ä¿å­˜GCSãƒ‘ã‚¹ã‚’å–å¾—
gcs_list_csv_path = os.getenv('GCS_LIST_CSV_PATH', '')

not_next = os.getenv('NOT_NEXT', 'False').lower() == 'true'

# å¯¾è±¡å¤–é–‹ç¤ºãƒªã‚¹ãƒˆ
exclude_title_path = os.path.join(os.path.dirname(__file__), '../data/exclude_title.csv')
exclude_title_df = pd.read_csv(exclude_title_path, header=None)
exclude_title = exclude_title_df.iloc[:,0].to_list()

# ä¿å­˜ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’ã‚’è¡Œã†
async def learning_from_save_data(
    target_date
    ):
    
    # ä¿å­˜ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    list_key = f'{gcs_list_csv_path}/{target_date}.json'
    data_list = googleapi.download_list(list_key)
    
    # æ ªä¾¡ãŒå…¥ã£ã¦ãªã„ã‚‚ã®ãŒã‚ã‚Œã°ä¿å­˜ã—ãªãŠã—
    if not not_next:    # ãƒ‡ãƒãƒƒã‚°ã§ã¯ã¨ã‚‰ãªã„
        data_list = re_get_stock_data(
            data_list,
            target_date
        )

    # 2. æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´é‡ä½œæˆ
    features = []
    targets = []
    documents = []
    debug_data = []
    
    total_count = 0
    
    print(f'ç·ä»¶æ•°ï¼š{len(data_list)}')
    
    if not_next:
        print('ãƒ‡ãƒãƒƒã‚°å®Ÿè¡Œ')
        test = [item['Link'] for item in data_list if not is_broken_text(item['Link'])]
        
        test_df = pd.DataFrame(test)
        test_df_path = os.path.join(os.path.dirname(__file__), '../data/documents.csv')
        test_df.to_csv(test_df_path, index=False)
        test_2 = summarize.summarize_in_parallel(test)
        
        test_2_df = pd.DataFrame(test_2)
        test_2_df_path = os.path.join(os.path.dirname(__file__), '../data/summaries.csv')
        test_2_df.to_csv(test_2_df_path, index=False)
        
        return
    

    for item in data_list:
        total_count += 1
        
        if is_broken_text(item['Link']):
            print(f'é–‹ç¤ºæ–‡ç« ãŒæ–‡å­—åŒ–ã‘ã—ã¦ã‚‹ãŸã‚ã‚¹ãƒ«ãƒ¼ï¼š{item['Code']}')
            continue
        
        # if not_next:    # ãƒ‡ãƒãƒƒã‚°ã§ã¯å…ˆé ­ã®ã¿
        #     if 2 <= total_count:
        #         break
        
        # éå»3ã‹æœˆã®æ ªä¾¡ãŸã¡ã‚’å–å¾—
        past_start_date = pd.to_datetime(item['Date'])
        past_stock_json, past_n225_json, past_growth_json = disclosure.get_amonth_finance(
            item['Code'],
            past_start_date,
            True
        )
        
        # JSONãƒ‡ãƒ¼ã‚¿ã‚’DataFrameã«å¤‰æ›
        if not past_stock_json:
            print(f'éå»æ ªä¾¡ãŒãªã„ãŸã‚ç‰¹å¾´é‡ãŒå…¥ã‚Œã‚Œãªã„ã‚¹ãƒ«ãƒ¼ï¼š{item['Code']}') 
            continue
        if not past_n225_json:
            print(f'éå»æ—¥çµŒæ ªä¾¡ãŒãªã„ãŸã‚ç‰¹å¾´é‡ãŒå…¥ã‚Œã‚Œãªã„ã‚¹ãƒ«ãƒ¼ï¼š{item['Code']}') 
            continue
        if not past_growth_json:
            print(f'éå»ã‚°ãƒ­ãƒ¼ã‚¹æ ªä¾¡ãŒãªã„ãŸã‚ç‰¹å¾´é‡ãŒå…¥ã‚Œã‚Œãªã„ã‚¹ãƒ«ãƒ¼ï¼š{item['Code']}') 
            continue
        
        df_stock = pd.DataFrame(json.loads(past_stock_json))
        df_nikkei = pd.DataFrame(json.loads(past_n225_json))
        df_mothers = pd.DataFrame(json.loads(past_growth_json))
        df_stock_targets = pd.DataFrame(json.loads(item['Stock']))  # çµæœç”¨
        
        if df_stock_targets.empty:
            print(f'äºˆæƒ³ç”¨æ ªä¾¡ãŒãªã„ãŸã‚ã‚¹ãƒ«ãƒ¼ï¼š{item['Code']}') 
            continue
        
        # è²¡å‹™æƒ…å ±ã‚’å–å¾—
        wk_stock = df_stock_targets[df_stock_targets['Date'] == target_date]
        if wk_stock.empty:
            print(f'{item['Code']}ï¼šå¯¾è±¡æ—¥ã®è²¡å‹™ãƒ‡ãƒ¼ã‚¿ãŒã¨ã‚Œãªã„ã®ã¯å¯¾è±¡å¤–')
            continue
        else :
            # ã„ã£ãŸã‚“æ™‚ä¾¡ç·é¡ã®ã¿ã®æŒ‡æ¨™ã¨ã™ã‚‹
            # aggregated_features = finance.get_financecapitalization_from_csv(
            #     item['Code'],
            #     target_date,
            #     wk_stock.iloc[0]['Open']
            # )
            # ã“ã£ã¡ã¯å„ç¨®ãƒ•ã‚¡ãƒ³ãƒ€ãƒ¡ãƒ³ã‚¿ãƒ«æŒ‡æ¨™ã‚‚
            aggregated_features = finance.get_finance_from_csv(
                item['Code'],
                target_date,
                wk_stock.iloc[0]['Open']
            )
            
        if not isinstance(aggregated_features, dict):
            print(f'{item['Code']}ï¼šè²¡å‹™ãƒ‡ãƒ¼ã‚¿ãŒã¨ã‚Œãªã„ã®ã¯å¯¾è±¡å¤–')
            continue

        # æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã®å¤‰æ›
        df_stock['Close'] = pd.to_numeric(df_stock['Close'], errors='coerce')
        df_stock['Volume'] = pd.to_numeric(df_stock['Volume'], errors='coerce')
        df_stock['Open'] = pd.to_numeric(df_stock['Open'], errors='coerce')
        df_stock['High'] = pd.to_numeric(df_stock['High'], errors='coerce')
        df_stock['Low'] = pd.to_numeric(df_stock['Low'], errors='coerce')

        df_nikkei['Close'] = pd.to_numeric(df_nikkei['Close'], errors='coerce')
        df_nikkei['Open'] = pd.to_numeric(df_nikkei['Open'], errors='coerce')
        df_nikkei['High'] = pd.to_numeric(df_nikkei['High'], errors='coerce')
        df_nikkei['Low'] = pd.to_numeric(df_nikkei['Low'], errors='coerce')
        df_nikkei['Volume'] = pd.to_numeric(df_nikkei['Volume'], errors='coerce')

        df_mothers['Close'] = pd.to_numeric(df_mothers['Close'], errors='coerce')
        df_mothers['Open'] = pd.to_numeric(df_mothers['Open'], errors='coerce')
        df_mothers['High'] = pd.to_numeric(df_mothers['High'], errors='coerce')
        df_mothers['Low'] = pd.to_numeric(df_mothers['Low'], errors='coerce')
        df_mothers['Volume'] = pd.to_numeric(df_mothers['Volume'], errors='coerce')

        df_stock_targets['Open'] = pd.to_numeric(df_stock_targets['Open'], errors='coerce')
        df_stock_targets['Close'] = pd.to_numeric(df_stock_targets['Close'], errors='coerce')

        # å¤‰åŒ–ç‡ï¼ˆãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸ï¼‰
        df_stock['CloseChange'] = df_stock['Close'].pct_change() * 100  
        df_nikkei['CloseChange'] = df_nikkei['Close'].pct_change() * 100  
        df_mothers['CloseChange'] = df_mothers['Close'].pct_change() * 100
        df_stock_targets['CloseChange'] = df_stock_targets['Close'].pct_change() * 100

        # é–‹ç¤ºæ—¥
        disclosure_date = pd.to_datetime(item["DateKey"])

        # é–‹ç¤ºæ—¥ã®Â±200æ—¥é–“ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º(éå»ã®ã‚„ã¤ã ã‹ã‚‰)
        df_stock['DaysSinceDisclosure'] = (pd.to_datetime(df_stock["Date"]) - disclosure_date).dt.days
        df_stock_recent = df_stock[(df_stock['DaysSinceDisclosure'] >= -200) & (df_stock['DaysSinceDisclosure'] <= 200)]

        df_nikkei['DaysSinceDisclosure'] = (pd.to_datetime(df_nikkei["Date"]) - disclosure_date).dt.days
        df_nikkei_recent = df_nikkei[df_nikkei['DaysSinceDisclosure'].isin(df_stock_recent['DaysSinceDisclosure'])]

        df_mothers['DaysSinceDisclosure'] = (pd.to_datetime(df_mothers["Date"]) - disclosure_date).dt.days
        df_mothers_recent = df_mothers[df_mothers['DaysSinceDisclosure'].isin(df_stock_recent['DaysSinceDisclosure'])]

        # --- ğŸ“Œ æ ªä¾¡ vs æŒ‡æ•°ã®ç›¸é–¢é–¢ä¿‚ ---
        NikkeiCorr = None
        if not df_stock_recent.empty and not df_nikkei_recent.empty:
            NikkeiCorr = df_stock_recent["CloseChange"].corr(df_nikkei_recent["CloseChange"])
            
        aggregated_features["NikkeiCorr"] = 0 if NikkeiCorr is None or pd.isna(NikkeiCorr) else NikkeiCorr 
        
        MothersCorr = None
        if not df_stock_recent.empty and not df_mothers_recent.empty:
            MothersCorr = df_stock_recent["CloseChange"].corr(df_mothers_recent["CloseChange"])
            
        aggregated_features["MothersCorr"] = 0 if MothersCorr is None or pd.isna(MothersCorr) else MothersCorr 

        # --- ğŸ“Œ ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™ ---
        df_stock['RSI'] = calculate_rsi(df_stock['Close'])  # RSIï¼ˆç›¸å¯¾åŠ›æŒ‡æ•°ï¼‰
        df_stock['MovingAverage50'] = df_stock['Close'].rolling(window=50, min_periods=1).mean()  # 50æ—¥ç§»å‹•å¹³å‡ç·š
        df_stock['MovingAverage200'] = df_stock['Close'].rolling(window=200, min_periods=1).mean()  # 200æ—¥ç§»å‹•å¹³å‡ç·š

        # RSIã€ç§»å‹•å¹³å‡ã‚’df_stock_recentã«ã‚‚åæ˜ 
        df_stock_recent['RSI'] = df_stock['RSI']
        df_stock_recent['MovingAverage50'] = df_stock['MovingAverage50']
        df_stock_recent['MovingAverage200'] = df_stock['MovingAverage200']

        aggregated_features.update({
            "RSI": df_stock_recent["RSI"].dropna().iloc[-1] if not df_stock_recent["RSI"].dropna().empty else 0,
            "MovingAverage50": df_stock_recent["MovingAverage50"].dropna().iloc[-1] if not df_stock_recent["MovingAverage50"].dropna().empty else 0,
            "MovingAverage200": df_stock_recent["MovingAverage200"].dropna().iloc[-1] if not df_stock_recent["MovingAverage200"].dropna().empty else 0,
        })

        # --- ğŸ“Œ ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£æŒ‡æ¨™ï¼ˆä¿®æ­£ç‰ˆï¼‰---
        df_stock['High-Low'] = df_stock['High'] - df_stock['Low']
        df_stock['High-ClosePrev'] = abs(df_stock['High'] - df_stock['Close'].shift(1))
        df_stock['Low-ClosePrev'] = abs(df_stock['Low'] - df_stock['Close'].shift(1))

        # True Rangeï¼ˆTRï¼‰ã®æ­£ã—ã„è¨ˆç®—
        df_stock['TR'] = df_stock[['High-Low', 'High-ClosePrev', 'Low-ClosePrev']].max(axis=1)

        # ATRï¼ˆ14æ—¥é–“ã®ç§»å‹•å¹³å‡ï¼‰
        df_stock['ATR'] = df_stock['TR'].rolling(window=14, min_periods=1).mean()

        # df_stock_recentã«ATRã‚’åæ˜ 
        df_stock_recent['ATR'] = df_stock['ATR']

        # ATRã®å¹³å‡ã‚’ç‰¹å¾´é‡ã¨ã—ã¦é›†ç´„
        aggregated_features["ATR"] = df_stock_recent["ATR"].dropna().mean() if not df_stock_recent["ATR"].dropna().empty else 0

        # --- ğŸ“Œ ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™ ---
        df_stock = calculate_macd(df_stock)  # MACD
        df_stock = calculate_bollinger_bands(df_stock)  # ãƒœãƒªãƒ³ã‚¸ãƒ£ãƒ¼ãƒãƒ³ãƒ‰
        df_stock = calculate_percent_r(df_stock)  # ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆãƒ¬ãƒ³ã‚¸
        df_stock = calculate_adx(df_stock)  # ADX

        # è¿½åŠ ã—ãŸæŒ‡æ¨™ã‚’é›†è¨ˆ
        aggregated_features.update({
            "MACD": df_stock['MACD'].dropna().iloc[-1] if not df_stock['MACD'].dropna().empty else 0,
            "Signal": df_stock['Signal'].dropna().iloc[-1] if not df_stock['Signal'].dropna().empty else 0,
            "UpperBand": df_stock['UpperBand'].dropna().iloc[-1] if not df_stock['UpperBand'].dropna().empty else 0,
            "LowerBand": df_stock['LowerBand'].dropna().iloc[-1] if not df_stock['LowerBand'].dropna().empty else 0,
            "PercentR": df_stock['PercentR'].dropna().iloc[-1] if not df_stock['PercentR'].dropna().empty else 0,
            "ADX": df_stock['ADX'].dropna().iloc[-1] if not df_stock['ADX'].dropna().empty else 0,
        })
        
        # Noneã€NaNã¯0ã«
        aggregated_features = {
            key : 0 if value is None or pd.isna(value) else value
            for key, value in aggregated_features.items()
        }

        # ç‰¹å¾´é‡ãƒªã‚¹ãƒˆã«è¿½åŠ 
        features.append(aggregated_features)
        
        # é–‹ç¤ºæ—¥ã‹ã‚‰å„ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®æ—¥æ•°å¾Œã®å¤‰å‹•ç‡ã‚’è¨ˆç®—(æ±‚ã‚ã‚‹çµæœ)
        days_list = [0, 3, 7, 14, 21, 28, 35, 42, 49]
        targets.append(get_last_valid_change_rate(df_stock_targets, days_list))
        
        # é–‹ç¤ºã‚’ã‚»ãƒƒãƒˆ
        documents.append(item["Link"])
        
        if not_next:    # ãƒ‡ãƒãƒƒã‚°ã§ã¯ã¨ã‚‰ãªã„
            debug_rates = get_last_valid_change_rate(df_stock_targets, days_list, True)
            # ãƒ‡ãƒãƒƒã‚°ç”¨ã«ã‚»ãƒƒãƒˆ
            debug_data.append({
                'Code': item['Code'],
                **aggregated_features,
                **debug_rates
            })
            
    # 1. é–‹ç¤ºæ–‡ç« ï¼ˆLinkï¼‰ã®ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡åŒ–
    # out_doc_path = os.path.join(os.path.dirname(__file__), '../data/documents.csv')
    # out_doc_df = pd.DataFrame(documents)
    # out_doc_df.to_csv(out_doc_path, index=False)
    
    document_summaries = summarize.summarize_in_parallel(documents)
    if not_next:
        summaries_df = pd.DataFrame(document_summaries)
        summaries_path = os.path.join(os.path.dirname(__file__), '../data/summaries.csv')
        summaries_df.to_csv(summaries_path, index=False)
    
    print(f'ç·ä»¶æ•°ï¼š{total_count} featuresä»¶æ•°ï¼š{len(features)} targetsä»¶æ•°ï¼š{len(targets)} documentsä»¶æ•°ï¼š{len(documents)}')
    
    # if not_next:    # ãƒ‡ãƒãƒƒã‚°ã§ã¯ã¨ã‚‰ãªã„
    #     # æŒ‡æ¨™ã®ä¸­èº«ã‚’ãƒ‡ãƒãƒƒã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦è½ã¨ã—ã¦ç¢ºèª
    #     debug_save = pd.DataFrame(debug_data)
    #     save_path = os.path.join(os.path.dirname(__file__), '../data/debug.csv')
        
    #     debug_save.to_csv(save_path, index=False)
        
    # LSTMå­¦ç¿’
    if not not_next:    # ãƒ‡ãƒãƒƒã‚°ã§ã¯ã¨ã‚‰ãªã„
        print(f'{target_date}ã‚’å­¦ç¿’')
        #lstm.lstm_learning(X_text, features, targets)
    
    # é–‹ç¤ºä¸€ã¤ãšã¤
    test = ''
    
# é–‹ç¤ºæ–‡ç« ã®æ–‡å­—åŒ–ã‘ãƒã‚§ãƒƒã‚¯(æ–‡å­—åŒ–ã‘ã—ã¦ã‚‹é–‹ç¤ºã¯ã‚¹ãƒ«ãƒ¼)
def is_broken_text(text):
    try:
        # æ–‡å­—åˆ—ã‚’ãƒã‚¤ãƒˆåˆ—ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã¦ã‹ã‚‰ã€ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¦ã¿ã‚‹
        byte_data = text.encode('utf-8')  # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        decoded_text = byte_data.decode('utf-8', errors='strict')  # ãƒ‡ã‚³ãƒ¼ãƒ‰

        # åˆ¶å¾¡æ–‡å­—ã‚„ä¸å¯è¦–æ–‡å­—ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ã‚’ç¢ºèª
        for char in decoded_text:
            if ord(char) < 32 or ord(char) == 127:  # åˆ¶å¾¡æ–‡å­—ã‚„ä¸å¯è¦–æ–‡å­—
                return True  # å£Šã‚Œã¦ã„ã‚‹ã¨åˆ¤å®š

        # ç‰¹å®šã®ç•°å¸¸ãªUnicodeæ–‡å­—ï¼ˆä¾‹ãˆã°ã€à¢†ï¼‰ã‚’ãƒã‚§ãƒƒã‚¯
        if re.search(r'[\u0500-\u05FF\u200B\u200C\u200D\u2060\u2061\u2062à¢†]', decoded_text):
            return True  # ç•°å¸¸ãªUnicodeæ–‡å­—ãŒã‚ã‚Œã°å£Šã‚Œã¦ã„ã‚‹ã¨åˆ¤å®š

        # æ—¥æœ¬èªãŒå«ã¾ã‚Œã¦ã„ãªã„å ´åˆã¯å£Šã‚ŒãŸæ–‡å­—åˆ—ã¨åˆ¤å®š
        if not re.search(r'[ã-ã‚“ã‚¡-ãƒ³ä¸€-é¾¯]', text):  # æ—¥æœ¬èªã®æ–‡å­—ç¯„å›²ã«ãƒãƒƒãƒã—ãªã„å ´åˆ
            return True

        return False  # å£Šã‚Œã¦ã„ãªã‘ã‚Œã° False

    except (UnicodeDecodeError, UnicodeEncodeError) as e:
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã¾ãŸã¯ãƒ‡ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼ã§å£Šã‚Œã¦ã„ã‚‹ã¨åˆ¤å®š
        return True
    
# æ ªä¾¡æƒ…å ±ãŒå…¥ã£ã¦ãªã‹ã£ãŸã‚‰ç™»éŒ²ã—ãªãŠã—
def re_get_stock_data(
    data_list,
    target_date
    ):
    
    # å…¨ã¦æ ªä¾¡ãŒå…¥ã£ã¦ã„ãŸã‚‰ãã®ã¾ã¾è¿”ã™
    modify_list = data_list
    
    # ï¼“ã‹æœˆå‰ãƒã‚§ãƒƒã‚¯
    org_date = datetime.today()
    before_90_date = org_date - timedelta(days=90)
    list_date = pd.to_datetime(target_date)
    
    df = pd.DataFrame(data_list)
    
    
    # å¤‰ãªã¨ã‚Œã‹ãŸã‚’ã—ã¦ã‚‹ã®ã§å–ã‚Šç›´ã™
    df[['Stock', 'N225', 'Growth']] = df.apply(
        lambda row: pd.Series(
            disclosure.get_amonth_finance(row['Code'], pd.to_datetime(row['Date']))
            ),
        axis=1
    )
    
    # æ›¸ãè¾¼ã¿ã€èª­ã¿è¾¼ã¿ã—ç›´ã—
    key = f'{gcs_list_csv_path}/{target_date}.json'
    googleapi.rewrite_list(
        df,
        key
    )
    
    modify_list = googleapi.download_list(key)
    
    # if list_date > before_90_date:
    #     # å¯¾è±¡æ—¥ãŒï¼“ã‹æœˆä»¥å†…ã®å ´åˆã¯æ ªä¾¡ã‚’åŸ‹ã‚ã‚‹ãŸã‚ã«å–ã‚Šç›´ã—
    #     df[['Stock', 'N225', 'Growth']] = df.apply(
    #         lambda row: pd.Series(
    #             disclosure.get_amonth_finance(row['Code'], pd.to_datetime(row['Date']))
    #             ),
    #         axis=1
    #     )
        
    #     # æ›¸ãè¾¼ã¿ã€èª­ã¿è¾¼ã¿ã—ç›´ã—
    #     key = f'{gcs_list_csv_path}/{target_date}.json'
    #     googleapi.rewrite_list(
    #         df,
    #         key
    #     )
        
    #     modify_list = googleapi.download_list(key)
    # else:
    #     # 90æ—¥ã‚ˆã‚Šå‰ã®ãƒ‡ãƒ¼ã‚¿ã¯åŸ‹ã¾ã£ã¦ãªã„ã¨ã“ã‚ã ã‘å–ã‚Šç›´ã—
        
    #     # å¯¾è±¡æ—¥ã®ä¸­ã«æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ãŒå…¥ã£ã¦ãªã„ã‚‚ã®ã‚’æŠ½å‡º
    #     mask = df['Stock'].apply(lambda x: isinstance(x, dict) and len(x) == 0)
        
    #     if mask.any():
    #         # å–å¾—å‡ºæ¥ã¦ãªã„ã‚‚ã®ãŒã‚ã‚Œã°å–å¾—ã—ã¦ä¿å­˜ã—ãªãŠã—
    #         df.loc[mask, ['Stock', 'N225', 'Growth']] = df.loc[mask].apply(
    #             lambda row: pd.Series(
    #                 disclosure.get_amonth_finance(row['Code'], pd.to_datetime(row['Date'])),
    #                 index=['Stock', 'N225', 'Growth']
    #                 ),
    #             axis=1
    #         )
            
    #         # ä¿å­˜ã—ãªãŠã—
    #         key = f'{gcs_list_csv_path}/{target_date}.json'
    #         googleapi.rewrite_list(
    #             df,
    #             key
    #         )
    #         # å–å¾—ã—ãªãŠã—
    #         modify_list = googleapi.download_list(key)
    
    return modify_list

# RSI (Relative Strength Index) ã‚’è¨ˆç®—
def calculate_rsi(series, window=14):
    delta = series.diff()
    gain = delta.where(delta > 0, 0)
    loss = -delta.where(delta < 0, 0)
    avg_gain = gain.rolling(window=window).mean()  # windowã«å¤‰æ›´
    avg_loss = loss.rolling(window=window).mean()  # windowã«å¤‰æ›´
    rs = avg_gain / avg_loss
    return 100 - (100 / (1 + rs))

# MACDã®è¨ˆç®—
def calculate_macd(df):
    # çŸ­æœŸã¨é•·æœŸã®EMAã‚’è¨ˆç®—
    df['EMA12'] = df['Close'].ewm(span=12, adjust=False).mean()  
    df['EMA26'] = df['Close'].ewm(span=26, adjust=False).mean()
    # MACDãƒ©ã‚¤ãƒ³
    df['MACD'] = df['EMA12'] - df['EMA26']
    # Signalãƒ©ã‚¤ãƒ³ï¼ˆMACDã®9æ—¥é–“ã®EMAï¼‰
    df['Signal'] = df['MACD'].ewm(span=9, adjust=False).mean()
    return df

# ãƒœãƒªãƒ³ã‚¸ãƒ£ãƒ¼ãƒãƒ³ãƒ‰ã®è¨ˆç®—
def calculate_bollinger_bands(df, window=20):
    df['MA20'] = df['Close'].rolling(window=window).mean()  # 20æ—¥ç§»å‹•å¹³å‡
    df['STD20'] = df['Close'].rolling(window=window).std()  # 20æ—¥æ¨™æº–åå·®
    df['UpperBand'] = df['MA20'] + (df['STD20'] * 2)  # ä¸Šé™
    df['LowerBand'] = df['MA20'] - (df['STD20'] * 2)  # ä¸‹é™
    return df

# ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆãƒ¬ãƒ³ã‚¸ï¼ˆ%Rï¼‰ã®è¨ˆç®—
def calculate_percent_r(df, window=14):
    df['HighestHigh'] = df['High'].rolling(window=window).max()
    df['LowestLow'] = df['Low'].rolling(window=window).min()
    df['PercentR'] = 100 * (df['HighestHigh'] - df['Close']) / (df['HighestHigh'] - df['LowestLow'])
    return df

# ADXã®è¨ˆç®—
def calculate_adx(df, window=14):
    # True Rangeã®è¨ˆç®—
    df['H-L'] = df['High'] - df['Low']
    df['H-PC'] = abs(df['High'] - df['Close'].shift(1))
    df['L-PC'] = abs(df['Low'] - df['Close'].shift(1))
    df['TR'] = df[['H-L', 'H-PC', 'L-PC']].max(axis=1)

    # Directional Movementã®è¨ˆç®—
    df['DM+'] = df['High'] - df['High'].shift(1)
    df['DM-'] = df['Low'].shift(1) - df['Low']

    df['DM+'] = np.where(df['DM+'] > 0, df['DM+'], 0)
    df['DM-'] = np.where(df['DM-'] > 0, df['DM-'], 0)

    df['ATR'] = df['TR'].rolling(window=window).mean()
    df['PDI'] = (df['DM+'].rolling(window=window).sum() / df['ATR']) * 100  # Positive Directional Index
    df['NDI'] = (df['DM-'].rolling(window=window).sum() / df['ATR']) * 100  # Negative Directional Index

    # ADXã®è¨ˆç®—
    df['ADX'] = (abs(df['PDI'] - df['NDI']) / (df['PDI'] + df['NDI'])).rolling(window=window).mean() * 100
    return df

# listã‹ã‚‰ã¾ã¨ã‚ã¦å¤‰åŒ–ç‡ã‚’å–å¾—ã™ã‚‹
def get_last_valid_change_rate(
    df,
    days_list,
    is_debug = False
    ):
    last_valid_value = None
    change_rates = {}
    
    for days in days_list:
        rate = calculate_change_rate(df, days, is_debug) if not df.empty else None
        if rate is not None:
            last_valid_value = rate  # æœ€å¾Œã®æœ‰åŠ¹ãªå€¤ã‚’æ›´æ–°
        else:
            rate = last_valid_value if last_valid_value is not None else -9999  # æ¬ æãªã‚‰æœ€å¾Œã®æœ‰åŠ¹å€¤ or -9999
        change_rates[f"ChangeRate_{days}"] = rate
    
    return change_rates

# æ ªä¾¡å¤‰å‹•ç‡ã‚’è¨ˆç®—ã™ã‚‹é–¢æ•°
def calculate_change_rate(
    df, 
    days,
    is_debug = False
    ):
    df_copy = df.copy()
    df_copy['ChangeRate'] = (
        df_copy['Close'].shift(-days) - df_copy['Close'] 
        if days != 0 else 
        df_copy['Close'] - df_copy['Open']
        ) / df_copy['Close'] * 100  
    
    # æœ€åˆã®è¡Œï¼ˆ0æ—¥ç›®ï¼‰ã® `days` æ—¥å¾Œã®å¤‰å‹•ç‡ã ã‘å–å¾—
    if len(df_copy) > days:
        if is_debug:
            # ãƒ‡ãƒãƒƒã‚°ã§æ¬²ã—ã„ã®ã¯æ ªä¾¡ã¨æ ªä¾¡ç‡
            return f'{df_copy['Close'].shift(-days).iloc[0]}:{df_copy['ChangeRate'].iloc[0]}'
        else:
            # é€šå¸¸ã¯æ ªä¾¡ç‡ã®ã¿
            return df_copy['ChangeRate'].iloc[0]
    else:
        return 
  
# é–‹ç¤ºæ–‡ç« ã¨ãã®è¦ç´„ã‚’å–å¾—  
async def get_summarize_list(
    target_date,
    is_financial_only
):
    # ä¿å­˜ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    list_key = f'{gcs_list_csv_path}/{target_date}.json'
    data_list = googleapi.download_list(list_key)
    
    # å–å¾—jsonã‚’
    data_df = pd.DataFrame(data_list)
    
    # ä¸è¦ã‚¿ã‚¤ãƒˆãƒ«ã‚’é™¤ã
    data_df = data_df[~data_df['Title'].str.contains('|'.join(exclude_title), na=False)]
    
    # æ±ºç®—ã¨ãã®ä»–ã§åˆ‡ã‚Šæ›¿ãˆã‚‹
    finance_Words = ['æ±ºç®—']
    if is_financial_only:
        # æ±ºç®—ã®ã¿
        data_df = data_df[data_df['Title'].str.contains('|'.join(finance_Words), na=False)]
    else:
        # æ±ºç®—ä»¥å¤–
        data_df = data_df[~data_df['Title'].str.contains('|'.join(finance_Words), na=False)]
    
    data_df = data_df[['Link']]
    
    data_df = data_df[~data_df['Link'].apply(is_broken_text)]
    #data_df = data_df[14:15]
    
    links = data_df['Link'].tolist()
    
    data_df['Summarize'] = summarize.summarize_in_parallel(links)
    
    return data_df.to_dict(orient="records")
