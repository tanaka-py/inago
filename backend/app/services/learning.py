import os
import json
import yfinance as yf
from datetime import datetime, timedelta
import pandas as pd
from . import googleapi, disclosure, finance, mlp, summarize, summarize_work
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import numpy as np
import re

import warnings # ã„ã£ãŸã‚“
# ç‰¹å®šã®è­¦å‘Šã‚’ç„¡è¦–ã™ã‚‹
warnings.filterwarnings('ignore', category=pd.errors.SettingWithCopyWarning)

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ä¼šç¤¾ãƒªã‚¹ãƒˆã®ä¿å­˜GCSãƒ‘ã‚¹ã‚’å–å¾—
gcs_list_csv_path = os.getenv('GCS_LIST_CSV_PATH', '')
gcs_work_csv_path = os.getenv('GCS_WORK_CSV_PATH', '')

is_debug = os.getenv('is_debug', 'False').lower() == 'true'

# å¯¾è±¡å¤–é–‹ç¤ºãƒªã‚¹ãƒˆ
exclude_title_path = os.path.join(os.path.dirname(__file__), '../data/exclude_title.csv')
exclude_title_df = pd.read_csv(exclude_title_path, header=None)
exclude_title = exclude_title_df.iloc[:,0].to_list()

# ä¿å­˜ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’ã‚’è¡Œã†
async def learning_from_save_data(
    target_date,
    work_load
    ):
    
    # ç‰¹å¾´é‡ç´ æ
    features = []
    targets = []
    documents = []
    document_summaries = []
    debug_data = []
        
    total_count = 0
    
    if work_load:
        # ä½œæ¥­ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        list_key = f'{gcs_work_csv_path}/{target_date}.json'
        data_list = googleapi.download_list(list_key)
        
        load_df = pd.DataFrame(data_list)
        
        features = load_df['features']
        targets = load_df['targets']
        document_summaries = load_df['document_summaries']
        
        total_count = len(features)
        
    else:
        
        # ä¿å­˜ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        list_key = f'{gcs_list_csv_path}/{target_date}.json'
        data_list = googleapi.download_list(list_key)
        
        # æ ªä¾¡ãŒå…¥ã£ã¦ãªã„ã‚‚ã®ãŒã‚ã‚Œã°ä¿å­˜ã—ãªãŠã—
        data_list = re_get_stock_data(
                data_list,
                target_date
            )
        
        # 2. æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´é‡ä½œæˆ
        print(f'ç·ä»¶æ•°ï¼š{len(data_list)}')
        
        for item in data_list:
            total_count += 1
            
            if is_broken_text(item['Link']):
                print(f'é–‹ç¤ºæ–‡ç« ãŒæ–‡å­—åŒ–ã‘ã—ã¦ã‚‹ãŸã‚ã‚¹ãƒ«ãƒ¼ï¼š{item['Code']}')
                continue
            
            # éå»3ã‹æœˆã®æ ªä¾¡ãŸã¡ã‚’å–å¾—
            past_start_date = pd.to_datetime(item['Date'])
            past_stock_json, past_n225_json, past_growth_json = disclosure.get_amonth_finance(
                item['Code'],
                past_start_date,
                True
            )
            
            # JSONãƒ‡ãƒ¼ã‚¿ã‚’DataFrameã«å¤‰æ›
            if not past_stock_json:
                print(f'éå»æ ªä¾¡ãŒãªã„ãŸã‚ç‰¹å¾´é‡ãŒå…¥ã‚Œã‚Œãªã„ã‚¹ãƒ«ãƒ¼ï¼š{item['Code']}') 
                continue
            if not past_n225_json:
                print(f'éå»æ—¥çµŒæ ªä¾¡ãŒãªã„ãŸã‚ç‰¹å¾´é‡ãŒå…¥ã‚Œã‚Œãªã„ã‚¹ãƒ«ãƒ¼ï¼š{item['Code']}') 
                continue
            if not past_growth_json:
                print(f'éå»ã‚°ãƒ­ãƒ¼ã‚¹æ ªä¾¡ãŒãªã„ãŸã‚ç‰¹å¾´é‡ãŒå…¥ã‚Œã‚Œãªã„ã‚¹ãƒ«ãƒ¼ï¼š{item['Code']}') 
                continue
            
            df_stock = pd.DataFrame(json.loads(past_stock_json))
            df_nikkei = pd.DataFrame(json.loads(past_n225_json))
            df_mothers = pd.DataFrame(json.loads(past_growth_json))
            df_stock_targets = pd.DataFrame(json.loads(item['Stock']))  # çµæœç”¨
            
            if df_stock_targets.empty:
                print(f'äºˆæƒ³ç”¨æ ªä¾¡ãŒãªã„ãŸã‚ã‚¹ãƒ«ãƒ¼ï¼š{item['Code']}') 
                continue
            
            # è²¡å‹™æƒ…å ±ã‚’å–å¾—
            wk_stock = df_stock_targets[df_stock_targets['Date'] == target_date]
            if wk_stock.empty:
                print(f'{item['Code']}ï¼šå¯¾è±¡æ—¥ã®è²¡å‹™ãƒ‡ãƒ¼ã‚¿ãŒã¨ã‚Œãªã„ã®ã¯å¯¾è±¡å¤–')
                continue
            
            code = item['Code']
            first_stock_open = wk_stock.iloc[0]['Open']
            # é–‹ç¤ºæ—¥
            disclosure_date = pd.to_datetime(item["DateKey"])
            
            # å„æŒ‡æ¨™ç‰¹å¾´é‡å–å¾—
            aggregated_features = calculate_aggregated_features(
                code,
                first_stock_open,
                df_stock, 
                df_nikkei, 
                df_mothers, 
                df_stock_targets,
                target_date, 
                disclosure_date
                )
            if aggregated_features is None:
                continue
            
            # é–‹ç¤ºæ—¥ã‹ã‚‰å„ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®æ—¥æ•°å¾Œã®å¤‰å‹•ç‡ã‚’è¨ˆç®—(æ±‚ã‚ã‚‹çµæœ)
            days_list = [0, 3, 7, 14, 21, 28, 35, 42, 49]
            rates = get_last_valid_change_rate(df_stock_targets, days_list)
            
            if any(rate is None or rate == -9999 for rate in rates):
                print(f'{item['Code']}ï¼šçµæœæ ªä¾¡å¤‰åŒ–ç‡ãŒã¨ã‚Œãªã„ã®ã¯å¯¾è±¡å¤–')
                continue
            
            targets.append(rates)

            # ç‰¹å¾´é‡ãƒªã‚¹ãƒˆã«è¿½åŠ 
            features.append(aggregated_features)
            
            # é–‹ç¤ºã‚’ã‚»ãƒƒãƒˆ
            documents.append(item["Link"])
        
        # è¦ç´„ã§ã¯ãªãã€å…ƒã®æ–‡ç« ã‹ã‚‰ã„ã‚‰ã‚“ã‚‚ã‚“ã‚’çœãã‚¹ã‚¿ã‚¤ãƒ«ã§
        document_summaries = [summarize.brushup_text(document) for document in documents]
    
    print(f'ç·ä»¶æ•°ï¼š{total_count} featuresä»¶æ•°ï¼š{len(features)} targetsä»¶æ•°ï¼š{len(targets)} documentsä»¶æ•°ï¼š{len(document_summaries)}')
    
    if len(targets) < 1:
        # å¯¾è±¡ãŒãªã„
        print('å¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãªã—ï¼šçµ‚äº†')
        return
    
    # DataFrameã«æ ¼ç´
    work_df = pd.DataFrame({
            'features': features,
            'targets': targets,
            'document_summaries': document_summaries
        })
    
    if work_load:
        # work_loadå¾Œã¯å­¦ç¿’
        # MLPå­¦ç¿’
        if not is_debug:    # ãƒ‡ãƒãƒƒã‚°ã§ã¯ã¨ã‚‰ãªã„
            #mlp.mlp_learning(document_summaries[0:1], features[0:1], targets[0:1])
            mlp.mlp_learning(document_summaries, features, targets)
    else:
        # consleã«é–‹ç¤ºã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹
        googleapi.rewrite_list(
                work_df,
                f'{gcs_work_csv_path}/work_data.json'
            )
        
    # è¡¨ç¤ºç”¨ã®å ´åˆã«ä¸€å¿œè¿”å´
    return work_df
    
# æ ªä¾¡äºˆæƒ³ä¸€è¦§å–å¾—
async def eval_target_list(
    target_date
    ):
    
    # ç‰¹å¾´é‡ç´ æ
    features = []
    documents = []
    document_summaries = []
        
    total_count = 0
    is_reload = False
    target_df = None
    
    # ã¾ãšã¯workã®ä¸­ã‚’è¦‹ã¦ã‹ã‚‰ãã£ã¡ã«ã‚ã‚‹å ´åˆã¯ãã£ã¡ã‹ã‚‰
    try:
        list_key = f'{gcs_work_csv_path}/{target_date}.json'
        data_list = googleapi.download_list(list_key)
        
        if not data_list:
            is_reload = True
        else:
            target_df = pd.DataFrame(data_list)
            features = target_df['features']
            document_summaries = target_df['document_summaries']
            
    except Exception as e:
        print("ä¿å­˜ãƒ‡ãƒ¼ã‚¿ãŒãªã„ãŸã‚å–å¾—ã—ãªãŠã—")
        is_reload = True
        
    # ä¿å­˜ãƒ‡ãƒ¼ã‚¿ãŒãªã„ãŸã‚å–å¾—ã—ãªãŠã—
    if is_reload:
        # ä¿å­˜ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        list_key = f'{gcs_list_csv_path}/{target_date}.json'
        data_list = googleapi.download_list(list_key)
        
        # 2. æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´é‡ä½œæˆ
        print(f'ç·ä»¶æ•°ï¼š{len(data_list)}')
        
        for item in data_list:
            total_count += 1
            
            if is_broken_text(item['Link']):
                print(f'é–‹ç¤ºæ–‡ç« ãŒæ–‡å­—åŒ–ã‘ã—ã¦ã‚‹ãŸã‚ã‚¹ãƒ«ãƒ¼ï¼š{item['Code']}')
                continue
            
            # éå»3ã‹æœˆã®æ ªä¾¡ãŸã¡ã‚’å–å¾—
            past_start_date = pd.to_datetime(item['Date'])
            past_stock_json, past_n225_json, past_growth_json = disclosure.get_amonth_finance(
                item['Code'],
                past_start_date,
                True
            )
            
            # JSONãƒ‡ãƒ¼ã‚¿ã‚’DataFrameã«å¤‰æ›
            if not past_stock_json:
                print(f'éå»æ ªä¾¡ãŒãªã„ãŸã‚ç‰¹å¾´é‡ãŒå…¥ã‚Œã‚Œãªã„ã‚¹ãƒ«ãƒ¼ï¼š{item['Code']}') 
                continue
            if not past_n225_json:
                print(f'éå»æ—¥çµŒæ ªä¾¡ãŒãªã„ãŸã‚ç‰¹å¾´é‡ãŒå…¥ã‚Œã‚Œãªã„ã‚¹ãƒ«ãƒ¼ï¼š{item['Code']}') 
                continue
            if not past_growth_json:
                print(f'éå»ã‚°ãƒ­ãƒ¼ã‚¹æ ªä¾¡ãŒãªã„ãŸã‚ç‰¹å¾´é‡ãŒå…¥ã‚Œã‚Œãªã„ã‚¹ãƒ«ãƒ¼ï¼š{item['Code']}') 
                continue
            
            df_stock = pd.DataFrame(json.loads(past_stock_json))
            df_nikkei = pd.DataFrame(json.loads(past_n225_json))
            df_mothers = pd.DataFrame(json.loads(past_growth_json))
            this_date_stock = disclosure.target_date_open_stock(past_start_date, item['Code'])
            df_stock_targets = pd.DataFrame(json.loads(this_date_stock))  # çµæœç”¨
            
            if df_stock_targets.empty:
                print(f'äºˆæƒ³ç”¨æ ªä¾¡ãŒãªã„ãŸã‚ã‚¹ãƒ«ãƒ¼ï¼š{item['Code']}') 
                continue
            
            # è²¡å‹™æƒ…å ±ã‚’å–å¾—
            wk_stock = df_stock_targets[df_stock_targets['Date'] == target_date]
            if wk_stock.empty:
                print(f'{item['Code']}ï¼šå¯¾è±¡æ—¥ã®è²¡å‹™ãƒ‡ãƒ¼ã‚¿ãŒã¨ã‚Œãªã„ã®ã¯å¯¾è±¡å¤–')
                continue
            
            code = item['Code']
            first_stock_open = wk_stock.iloc[0]['Open']
            # é–‹ç¤ºæ—¥
            disclosure_date = pd.to_datetime(item["DateKey"])
            
            # å„æŒ‡æ¨™ç‰¹å¾´é‡å–å¾—
            aggregated_features = calculate_aggregated_features(
                code,
                first_stock_open,
                df_stock, 
                df_nikkei, 
                df_mothers, 
                df_stock_targets,
                target_date, 
                disclosure_date
                )
            if aggregated_features is None:
                continue

            # ç‰¹å¾´é‡ãƒªã‚¹ãƒˆã«è¿½åŠ 
            features.append(aggregated_features)
            
            # é–‹ç¤ºã‚’ã‚»ãƒƒãƒˆ
            documents.append(item["Link"])
        
        # è¦ç´„ã§ã¯ãªãã€å…ƒã®æ–‡ç« ã‹ã‚‰ã„ã‚‰ã‚“ã‚‚ã‚“ã‚’çœãã‚¹ã‚¿ã‚¤ãƒ«ã§
        document_summaries = [summarize.brushup_text(document) for document in documents]
        
        # DataFrameã«æ ¼ç´
        target_df = pd.DataFrame({
                'features': features,
                'document_summaries': document_summaries
            })
        # consleã«é–‹ç¤ºã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹
        googleapi.rewrite_list(
                target_df,
                f'{gcs_work_csv_path}/{target_date}.json'
            )
        
  
    print(f'ç·ä»¶æ•°ï¼š{total_count} featuresä»¶æ•°ï¼š{len(features)} documentsä»¶æ•°ï¼š{len(document_summaries)}')
    
    model = mlp.model_load()
    
    if model is None:
        print('modelãŒèª­ã¿è¾¼ã‚ãªã„ãŸã‚äºˆæ¸¬ãŒå‡ºæ¥ã¾ã›ã‚“')
    else:
        # äºˆæƒ³å¤‰åŒ–ç‡ã‚’å–å¾—
        target_df['targets'] = target_df.apply(lambda row: mlp.targets_from_model(model, row['document_summaries'], row['features']), axis=1) 
    
    return target_df.to_dict(orient="records")
    
       
# æŒ‡æ¨™ç‰¹å¾´é‡(feature) 
# featuresï¼šé–‹ç¤ºæ–‡ç« ãŒå‡ºãŸæ™‚ç‚¹ã®å„æŒ‡æ¨™
#           'EPS'
#           'ROE'
#           'PER'
#           'PBR'
#           'Market Capitalization Log'
#           'NikkeiCorr'
#           'MothersCorr'
#           'RSI'
#           'MovingAverage50'
#           'MovingAverage200'
#           'ATR'
#           'MACD'
#           'Signal'
#           'UpperBand'
#           'LowerBand'
#           'PercentR'
#           'ADX'
def calculate_aggregated_features(
    code,
    first_stock_open,
    df_stock, 
    df_nikkei, 
    df_mothers, 
    df_stock_targets, 
    target_date, 
    disclosure_date
    ):
    # ã“ã£ã¡ã¯å„ç¨®ãƒ•ã‚¡ãƒ³ãƒ€ãƒ¡ãƒ³ã‚¿ãƒ«æŒ‡æ¨™ã‚‚
    aggregated_features = finance.get_finance_from_csv(
        code,
        target_date,
        first_stock_open
    )
        
    if not isinstance(aggregated_features, dict):
        print(f'{code}ï¼šè²¡å‹™ãƒ‡ãƒ¼ã‚¿ãŒã¨ã‚Œãªã„ã®ã¯å¯¾è±¡å¤–')
        return None

    # æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã®å¤‰æ›
    # æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã®å¤‰æ›ã‚’ä¸€æ°—ã«
    for col in ['Close', 'Volume', 'Open', 'High', 'Low']:
        df_stock[col] = pd.to_numeric(df_stock[col], errors='coerce')
        df_nikkei[col] = pd.to_numeric(df_nikkei[col], errors='coerce')
        df_mothers[col] = pd.to_numeric(df_mothers[col], errors='coerce')

    df_stock_targets['Open'] = pd.to_numeric(df_stock_targets['Open'], errors='coerce')
    df_stock_targets['Close'] = pd.to_numeric(df_stock_targets['Close'], errors='coerce')

    # å¤‰åŒ–ç‡ï¼ˆãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸ï¼‰
    df_stock['CloseChange'] = df_stock['Close'].pct_change() * 100  
    df_nikkei['CloseChange'] = df_nikkei['Close'].pct_change() * 100  
    df_mothers['CloseChange'] = df_mothers['Close'].pct_change() * 100
    df_stock_targets['CloseChange'] = df_stock_targets['Close'].pct_change() * 100

    # é–‹ç¤ºæ—¥ã®Â±200æ—¥é–“ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º(éå»ã®ã‚„ã¤ã ã‹ã‚‰)
    df_stock['DaysSinceDisclosure'] = (pd.to_datetime(df_stock["Date"]) - disclosure_date).dt.days
    df_stock_recent = df_stock[(df_stock['DaysSinceDisclosure'] >= -200) & (df_stock['DaysSinceDisclosure'] <= 200)]

    df_nikkei['DaysSinceDisclosure'] = (pd.to_datetime(df_nikkei["Date"]) - disclosure_date).dt.days
    df_nikkei_recent = df_nikkei[df_nikkei['DaysSinceDisclosure'].isin(df_stock_recent['DaysSinceDisclosure'])]

    df_mothers['DaysSinceDisclosure'] = (pd.to_datetime(df_mothers["Date"]) - disclosure_date).dt.days
    df_mothers_recent = df_mothers[df_mothers['DaysSinceDisclosure'].isin(df_stock_recent['DaysSinceDisclosure'])]

    # --- ğŸ“Œ æ ªä¾¡ vs æŒ‡æ•°ã®ç›¸é–¢é–¢ä¿‚ ---
    NikkeiCorr = None
    if not df_stock_recent.empty and not df_nikkei_recent.empty:
        NikkeiCorr = df_stock_recent["CloseChange"].corr(df_nikkei_recent["CloseChange"])
        
    aggregated_features["NikkeiCorr"] = 0 if NikkeiCorr is None or pd.isna(NikkeiCorr) else NikkeiCorr 
    
    MothersCorr = None
    if not df_stock_recent.empty and not df_mothers_recent.empty:
        MothersCorr = df_stock_recent["CloseChange"].corr(df_mothers_recent["CloseChange"])
        
    aggregated_features["MothersCorr"] = 0 if MothersCorr is None or pd.isna(MothersCorr) else MothersCorr 

    # --- ğŸ“Œ ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™ ---
    df_stock['RSI'] = calculate_rsi(df_stock['Close'])  # RSIï¼ˆç›¸å¯¾åŠ›æŒ‡æ•°ï¼‰
    df_stock['MovingAverage50'] = df_stock['Close'].rolling(window=50, min_periods=1).mean()  # 50æ—¥ç§»å‹•å¹³å‡ç·š
    df_stock['MovingAverage200'] = df_stock['Close'].rolling(window=200, min_periods=1).mean()  # 200æ—¥ç§»å‹•å¹³å‡ç·š

    # RSIã€ç§»å‹•å¹³å‡ã‚’df_stock_recentã«ã‚‚åæ˜ 
    df_stock_recent['RSI'] = df_stock['RSI']
    df_stock_recent['MovingAverage50'] = df_stock['MovingAverage50']
    df_stock_recent['MovingAverage200'] = df_stock['MovingAverage200']

    aggregated_features.update({
        "RSI": df_stock_recent["RSI"].dropna().iloc[-1] if not df_stock_recent["RSI"].dropna().empty else 0,
        "MovingAverage50": df_stock_recent["MovingAverage50"].dropna().iloc[-1] if not df_stock_recent["MovingAverage50"].dropna().empty else 0,
        "MovingAverage200": df_stock_recent["MovingAverage200"].dropna().iloc[-1] if not df_stock_recent["MovingAverage200"].dropna().empty else 0,
    })

    # --- ğŸ“Œ ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£æŒ‡æ¨™ï¼ˆä¿®æ­£ç‰ˆï¼‰---
    df_stock['High-Low'] = df_stock['High'] - df_stock['Low']
    df_stock['High-ClosePrev'] = abs(df_stock['High'] - df_stock['Close'].shift(1))
    df_stock['Low-ClosePrev'] = abs(df_stock['Low'] - df_stock['Close'].shift(1))

    # True Rangeï¼ˆTRï¼‰ã®æ­£ã—ã„è¨ˆç®—
    df_stock['TR'] = df_stock[['High-Low', 'High-ClosePrev', 'Low-ClosePrev']].max(axis=1)

    # ATRï¼ˆ14æ—¥é–“ã®ç§»å‹•å¹³å‡ï¼‰
    df_stock['ATR'] = df_stock['TR'].rolling(window=14, min_periods=1).mean()

    # df_stock_recentã«ATRã‚’åæ˜ 
    df_stock_recent['ATR'] = df_stock['ATR']

    # ATRã®å¹³å‡ã‚’ç‰¹å¾´é‡ã¨ã—ã¦é›†ç´„
    aggregated_features["ATR"] = df_stock_recent["ATR"].dropna().mean() if not df_stock_recent["ATR"].dropna().empty else 0

    # --- ğŸ“Œ ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™ ---
    df_stock = calculate_macd(df_stock)  # MACD
    df_stock = calculate_bollinger_bands(df_stock)  # ãƒœãƒªãƒ³ã‚¸ãƒ£ãƒ¼ãƒãƒ³ãƒ‰
    df_stock = calculate_percent_r(df_stock)  # ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆãƒ¬ãƒ³ã‚¸
    df_stock = calculate_adx(df_stock)  # ADX

    # è¿½åŠ ã—ãŸæŒ‡æ¨™ã‚’é›†è¨ˆ
    aggregated_features.update({
        "MACD": df_stock['MACD'].dropna().iloc[-1] if not df_stock['MACD'].dropna().empty else 0,
        "Signal": df_stock['Signal'].dropna().iloc[-1] if not df_stock['Signal'].dropna().empty else 0,
        "UpperBand": df_stock['UpperBand'].dropna().iloc[-1] if not df_stock['UpperBand'].dropna().empty else 0,
        "LowerBand": df_stock['LowerBand'].dropna().iloc[-1] if not df_stock['LowerBand'].dropna().empty else 0,
        "PercentR": df_stock['PercentR'].dropna().iloc[-1] if not df_stock['PercentR'].dropna().empty else 0,
        "ADX": df_stock['ADX'].dropna().iloc[-1] if not df_stock['ADX'].dropna().empty else 0,
    })
    
    # Noneã€NaNã¯0ã«
    aggregated_features = {
        key : 0 if value is None or pd.isna(value) else value
        for key, value in aggregated_features.items()
    }

    return aggregated_features

    
    
# é–‹ç¤ºæ–‡ç« ã®æ–‡å­—åŒ–ã‘ãƒã‚§ãƒƒã‚¯(æ–‡å­—åŒ–ã‘ã—ã¦ã‚‹é–‹ç¤ºã¯ã‚¹ãƒ«ãƒ¼)
def is_broken_text(text):
    try:
        # æ–‡å­—åˆ—ã‚’ãƒã‚¤ãƒˆåˆ—ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã¦ã‹ã‚‰ã€ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¦ã¿ã‚‹
        byte_data = text.encode('utf-8')  # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        decoded_text = byte_data.decode('utf-8', errors='strict')  # ãƒ‡ã‚³ãƒ¼ãƒ‰

        # åˆ¶å¾¡æ–‡å­—ã‚„ä¸å¯è¦–æ–‡å­—ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ã‚’ç¢ºèª
        for char in decoded_text:
            if ord(char) < 32 or ord(char) == 127:  # åˆ¶å¾¡æ–‡å­—ã‚„ä¸å¯è¦–æ–‡å­—
                return True  # å£Šã‚Œã¦ã„ã‚‹ã¨åˆ¤å®š

        # ç‰¹å®šã®ç•°å¸¸ãªUnicodeæ–‡å­—ï¼ˆä¾‹ãˆã°ã€à¢†ï¼‰ã‚’ãƒã‚§ãƒƒã‚¯
        if re.search(r'[\u0500-\u05FF\u200B\u200C\u200D\u2060\u2061\u2062à¢†]', decoded_text):
            return True  # ç•°å¸¸ãªUnicodeæ–‡å­—ãŒã‚ã‚Œã°å£Šã‚Œã¦ã„ã‚‹ã¨åˆ¤å®š

        # æ—¥æœ¬èªãŒå«ã¾ã‚Œã¦ã„ãªã„å ´åˆã¯å£Šã‚ŒãŸæ–‡å­—åˆ—ã¨åˆ¤å®š
        if not re.search(r'[ã-ã‚“ã‚¡-ãƒ³ä¸€-é¾¯]', text):  # æ—¥æœ¬èªã®æ–‡å­—ç¯„å›²ã«ãƒãƒƒãƒã—ãªã„å ´åˆ
            return True

        return False  # å£Šã‚Œã¦ã„ãªã‘ã‚Œã° False

    except (UnicodeDecodeError, UnicodeEncodeError) as e:
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã¾ãŸã¯ãƒ‡ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼ã§å£Šã‚Œã¦ã„ã‚‹ã¨åˆ¤å®š
        return True
    
# æ ªä¾¡æƒ…å ±ãŒå…¥ã£ã¦ãªã‹ã£ãŸã‚‰ç™»éŒ²ã—ãªãŠã—
def re_get_stock_data(
    data_list,
    target_date
    ):
    
    # å…¨ã¦æ ªä¾¡ãŒå…¥ã£ã¦ã„ãŸã‚‰ãã®ã¾ã¾è¿”ã™
    modify_list = data_list
    
    # ï¼“ã‹æœˆå‰ãƒã‚§ãƒƒã‚¯
    org_date = datetime.today()
    before_90_date = org_date - timedelta(days=90)
    list_date = pd.to_datetime(target_date)
    
    df = pd.DataFrame(data_list)
    
    
    # å¤‰ãªã¨ã‚Œã‹ãŸã‚’ã—ã¦ã‚‹ã®ã§å–ã‚Šç›´ã™
    df[['Stock', 'N225', 'Growth']] = df.apply(
        lambda row: pd.Series(
            disclosure.get_amonth_finance(row['Code'], pd.to_datetime(row['Date']))
            ),
        axis=1
    )
    
    # æ›¸ãè¾¼ã¿ã€èª­ã¿è¾¼ã¿ã—ç›´ã—
    key = f'{gcs_list_csv_path}/{target_date}.json'
    googleapi.rewrite_list(
        df,
        key
    )
    
    modify_list = googleapi.download_list(key)
    
    return modify_list

# RSI (Relative Strength Index) ã‚’è¨ˆç®—
def calculate_rsi(series, window=14):
    delta = series.diff()
    gain = delta.where(delta > 0, 0)
    loss = -delta.where(delta < 0, 0)
    avg_gain = gain.rolling(window=window).mean()  # windowã«å¤‰æ›´
    avg_loss = loss.rolling(window=window).mean()  # windowã«å¤‰æ›´
    rs = avg_gain / avg_loss
    return 100 - (100 / (1 + rs))

# MACDã®è¨ˆç®—
def calculate_macd(df):
    # çŸ­æœŸã¨é•·æœŸã®EMAã‚’è¨ˆç®—
    df['EMA12'] = df['Close'].ewm(span=12, adjust=False).mean()  
    df['EMA26'] = df['Close'].ewm(span=26, adjust=False).mean()
    # MACDãƒ©ã‚¤ãƒ³
    df['MACD'] = df['EMA12'] - df['EMA26']
    # Signalãƒ©ã‚¤ãƒ³ï¼ˆMACDã®9æ—¥é–“ã®EMAï¼‰
    df['Signal'] = df['MACD'].ewm(span=9, adjust=False).mean()
    return df

# ãƒœãƒªãƒ³ã‚¸ãƒ£ãƒ¼ãƒãƒ³ãƒ‰ã®è¨ˆç®—
def calculate_bollinger_bands(df, window=20):
    df['MA20'] = df['Close'].rolling(window=window).mean()  # 20æ—¥ç§»å‹•å¹³å‡
    df['STD20'] = df['Close'].rolling(window=window).std()  # 20æ—¥æ¨™æº–åå·®
    df['UpperBand'] = df['MA20'] + (df['STD20'] * 2)  # ä¸Šé™
    df['LowerBand'] = df['MA20'] - (df['STD20'] * 2)  # ä¸‹é™
    return df

# ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆãƒ¬ãƒ³ã‚¸ï¼ˆ%Rï¼‰ã®è¨ˆç®—
def calculate_percent_r(df, window=14):
    df['HighestHigh'] = df['High'].rolling(window=window).max()
    df['LowestLow'] = df['Low'].rolling(window=window).min()
    df['PercentR'] = 100 * (df['HighestHigh'] - df['Close']) / (df['HighestHigh'] - df['LowestLow'])
    return df

# ADXã®è¨ˆç®—
def calculate_adx(df, window=14):
    # True Rangeã®è¨ˆç®—
    df['H-L'] = df['High'] - df['Low']
    df['H-PC'] = abs(df['High'] - df['Close'].shift(1))
    df['L-PC'] = abs(df['Low'] - df['Close'].shift(1))
    df['TR'] = df[['H-L', 'H-PC', 'L-PC']].max(axis=1)

    # Directional Movementã®è¨ˆç®—
    df['DM+'] = df['High'] - df['High'].shift(1)
    df['DM-'] = df['Low'].shift(1) - df['Low']

    df['DM+'] = np.where(df['DM+'] > 0, df['DM+'], 0)
    df['DM-'] = np.where(df['DM-'] > 0, df['DM-'], 0)

    df['ATR'] = df['TR'].rolling(window=window).mean()
    df['PDI'] = (df['DM+'].rolling(window=window).sum() / df['ATR']) * 100  # Positive Directional Index
    df['NDI'] = (df['DM-'].rolling(window=window).sum() / df['ATR']) * 100  # Negative Directional Index

    # ADXã®è¨ˆç®—
    df['ADX'] = (abs(df['PDI'] - df['NDI']) / (df['PDI'] + df['NDI'])).rolling(window=window).mean() * 100
    return df

# listã‹ã‚‰ã¾ã¨ã‚ã¦å¤‰åŒ–ç‡ã‚’å–å¾—ã™ã‚‹
def get_last_valid_change_rate(
    df,
    days_list,
    is_debug = False
    ):
    last_valid_value = None
    change_rates = {}
    
    for days in days_list:
        rate = calculate_change_rate(df, days, is_debug) if not df.empty else None
        if rate is not None:
            last_valid_value = rate  # æœ€å¾Œã®æœ‰åŠ¹ãªå€¤ã‚’æ›´æ–°
        else:
            rate = last_valid_value if last_valid_value is not None else -9999  # æ¬ æãªã‚‰æœ€å¾Œã®æœ‰åŠ¹å€¤ or -9999
        change_rates[f"ChangeRate_{days}"] = rate
    
    return change_rates

# æ ªä¾¡å¤‰å‹•ç‡ã‚’è¨ˆç®—ã™ã‚‹é–¢æ•°
def calculate_change_rate(
    df, 
    days,
    is_debug = False
    ):
    df_copy = df.copy()
    df_copy['ChangeRate'] = (
        df_copy['Close'].shift(-days) - df_copy['Close'] 
        if days != 0 else 
        df_copy['Close'] - df_copy['Open']
        ) / df_copy['Close'] * 100  
    
    # æœ€åˆã®è¡Œï¼ˆ0æ—¥ç›®ï¼‰ã® `days` æ—¥å¾Œã®å¤‰å‹•ç‡ã ã‘å–å¾—
    if len(df_copy) > days:
        if is_debug:
            # ãƒ‡ãƒãƒƒã‚°ã§æ¬²ã—ã„ã®ã¯æ ªä¾¡ã¨æ ªä¾¡ç‡
            return f'{df_copy['Close'].shift(-days).iloc[0]}:{df_copy['ChangeRate'].iloc[0]}'
        else:
            # é€šå¸¸ã¯æ ªä¾¡ç‡ã®ã¿
            return df_copy['ChangeRate'].iloc[0]
    else:
        return 
  
# å­¦ç¿’å‰ãƒ‡ãƒ¼ã‚¿ç¢ºèª
async def get_work_data_list(
):
    # ä¿å­˜ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    list_key = f'{gcs_work_csv_path}/work_data.json'
    data_list = googleapi.download_list(list_key)
    
    # å–å¾—jsonã‚’
    data_df = pd.DataFrame(data_list)
    
    return data_df.to_dict(orient="records")

# é–‹ç¤ºæ–‡ç« ã¨ãã®è¦ç´„ã‚’å–å¾—  ãƒ‡ãƒãƒƒã‚°ç¢ºèªç”¨
async def get_summarize_list(
):
    # ç¾åœ¨ä½œæ¥­ä¸­ã®å¯¾è±¡æ—¥ã‚’å–å¾—
    target_date_path = os.path.join(os.path.dirname(__file__), '../data/next_learndate.txt')
    if os.path.exists(target_date_path):
        with open(target_date_path, 'r', encoding='utf-8') as f:
            target_date = f.read().strip()
    
    # ä¿å­˜ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    list_key = f'{gcs_list_csv_path}/{target_date}.json'
    data_list = googleapi.download_list(list_key)
    
    # å–å¾—jsonã‚’
    data_df = pd.DataFrame(data_list)
    data_df = data_df[['Link']]
    
    data_df = data_df[~data_df['Link'].apply(is_broken_text)]
    #data_df = data_df[0:1]
    
    links = data_df['Link'].tolist()
    
    #data_df['Summarize'] = summarize_work.summarize_in_parallel(links)
    #data_df['Summarize'] = summarize.brush_in_parallel(links)
    data_df['Summarize'] = [summarize.brushup_text(link) for link in links]
    
    return data_df.to_dict(orient="records")

# ä¸€è¦§ã‹ã‚‰å–å¾—ã—ãŸ
async def upload_disclosure_from_list(
    df,
    is_today = False
):
    # ä¿å­˜ç”¨ã®æ—¥ä»˜ã®ã¿ã®ã‚­ãƒ¼å–å¾—
    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
    df['DateKey'] = df['Date'].dt.strftime('%Y-%m-%d')
    
    # NaNã‚’Noneã«å¤‰æ›
    df = df.where(pd.notnull(df), None)
    
    # æ—¥ä»˜ãŒãªã„ã‚‚ã®ã¯ã“ã“ã§çœã
    df = df[df['Date'].notna()]
    
    # ä¸€ã‹æœˆã®æ ªä¾¡æƒ…å ±ã‚’å–å¾—
    if is_today:
        # æœ¬æ—¥ã®å ´åˆãƒ‡ãƒ¼ã‚¿ãŒãªã„ã®ã§ç©ºã§
        df[['Stock', 'N225', 'Growth']] = df.apply(lambda x: pd.Series({'Stock':{}, 'N225':{}, 'Growth':{}}), axis=1)
    else:
        # æ ªä¾¡ã‚»ãƒƒãƒˆ
        df[['Stock', 'N225', 'Growth']] = df.apply(
            lambda row: pd.Series(disclosure.get_amonth_finance(row['Code'], row['Date'])), axis=1
            )
    
    # æ—¥ä»˜åˆ¥ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    grouped_dfs = [group for _, group in df.groupby('DateKey')]
    for item_df in grouped_dfs:
        date_key = item_df['DateKey'].iloc[0]
        
        #jsonã‚’GCSã«ä¿å­˜
        if is_today:
            # å®Œå…¨ä¸Šæ›¸ã
            googleapi.rewrite_list(
                item_df,
                f'{gcs_list_csv_path}/{date_key}.json'
            )
        else:
            # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãã®ã¾ã¾ã§ä¿å­˜ã™ã‚‹
            googleapi.upload_list(
                item_df,
                f'{gcs_list_csv_path}/{date_key}.json'
            )
    